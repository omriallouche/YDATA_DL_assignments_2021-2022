{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two_layer_net.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BU6-Ftb3b1r8"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementing a Neural Network for CIFAR dataset\n",
        "In this exercise, you should change H.W_1 to work on CIFAR dataset.\n",
        "\n",
        "CIFAR is a dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n",
        "\n",
        "We will have the same components as were in the previous exercise:\n",
        "- Loading the data and visualize some of the images\n",
        "- Training the model. Instead of loading a pre-trained model, we will define a new very basic architecture.\n"
      ]
    },
    {
      "metadata": {
        "id": "ovTPl1D_b1sB"
      },
      "cell_type": "markdown",
      "source": [
        "## 1) Loading CIFAR dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should load the CIFAR dataset. You can use [the pytorch CIFAR tutorial](https://https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). As was done in the previous exercise, you should define a dictionary of ``dataloaders``. For now, no need to define pre-process to the data. \n",
        "Please stick to the API of H.W_1, create a dataloaders disctionary ``{'train','val'}``\n",
        "\n",
        "You can use batch_size = 128"
      ],
      "metadata": {
        "id": "3MnBLXdCPtzB"
      }
    },
    {
      "metadata": {
        "id": "8bAzo6A_b1sC"
      },
      "cell_type": "code",
      "source": [
        "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "### Enter you code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the availability of a GPU, and use CPU otherwise\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "yRRXBXkvpAwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you manage to load the CIFAR data correctly, you should run the next three cells and see some of the images.**"
      ],
      "metadata": {
        "id": "4AZrcZPki7A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize some of the training images"
      ],
      "metadata": {
        "id": "sfq03MwaTJSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset the size of the images are 32x32. \n",
        "\n",
        "It is important to visualize the data in order to see that we load them correctly (maybe we need to divide by 255? should we make channel permutations?) \n",
        "\n",
        "The code below was taken from H.W_1"
      ],
      "metadata": {
        "id": "A2oILjGNTVhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "inputs = inputs[0:16]\n",
        "classes = classes[0:16]"
      ],
      "metadata": {
        "id": "va_zicFdQi92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UkqNNRbOb1sF"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    fig = plt.figure(figsize=(5,3), dpi=300)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "       plt.title(title, fontsize=5)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs, nrow=8)\n",
        "imshow(out, title='\\n'.join([class_names[x] for x in classes]))"
      ],
      "metadata": {
        "id": "P3RLKeR5TiKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3hs_Z3Pib1sN"
      },
      "cell_type": "markdown",
      "source": [
        "## A 2-layer Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "HN8tSjQFb1sO"
      },
      "cell_type": "markdown",
      "source": [
        "We will define 2-fully-connected-layer network using Sequential module. We have two hidden layers: the first one has 100 neurons and the second one has 20 neurons. The last layer is the output that has 10 elements.\n",
        "\n",
        "Please pay attention that we take images after flattening (line 15 below)."
      ]
    },
    {
      "metadata": {
        "id": "yNhW2McRb1sP"
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(32*32*3, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 10),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "pf8pYxtDosaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pay attention, in case you change something in the parameters, the data, data loaders, image transform, optimizations and more...you should load the model again so it will start from random weights and not from the last training**.\n",
        "\n",
        "**You should run again the line** ``model = NeuralNetwork().to(device)``"
      ],
      "metadata": {
        "id": "1yLN2disJaYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The next two cells (defining the loss and the training loop) were copied from last exercise. \n"
      ],
      "metadata": {
        "id": "h26PigiUkWFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If a GPU is available, make the model use it\n",
        "model = model.to(device)\n",
        "\n",
        "# For a multi-class problem, you'd usually prefer CrossEntropyLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Stochastic Gradient Descent as the optimizer, with a learning rate of 0.0001 and momentum\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "wXtEcNnKo6Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # Set model to training mode. \n",
        "                model.train()  \n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()   \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    outputs = model(inputs) # apply the model to the inputs. The output is the softmax probability of each class\n",
        "                    _, preds = torch.max(outputs, 1) # \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step() # Adapt the optimizer\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()  \n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "vntk-kp5psrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=num_epochs)\n"
      ],
      "metadata": {
        "id": "mBSK3TR9pyWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can continue after you see your model learns something (the training loss is decreasing)."
      ],
      "metadata": {
        "id": "hhsQNyxbkeOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Losses in Pytorch\n",
        "\n",
        "Now, when you manage to run the training loop, let's understand the loss definition. PyTorch provides losses such as cross-entropy loss (`nn.CrossEntropyLoss`) that has just been used. You'll usually see the loss assigned to `criterion`. For classification problems such as CIFAR, we use the softmax function to predict class probabilities. With a softmax output, we need to use cross-entropy as the loss. To actually calculate the loss, we first define the criterion, then pass in it the output of your network and the correct labels.\n",
        "\n",
        "Really important note. Look at [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
        "\n",
        "> This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
        ">\n",
        "> The input is expected to contain scores for each class.\n",
        "\n",
        "It means that we need to pass in `nn.CrossEntropyLoss` the raw output of our network into, not the output of the softmax function. This raw output is usually called the *logits* or *scores*. We use the logits because softmax gives you probabilities, which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). It's usually better to avoid doing calculations with probabilities, and generally we use log-probabilities.\n",
        "\n",
        "**Exercise:** Change the network above to output the log-softmax and calculate the loss using the negative log-likelihood loss. Note that for `nn.LogSoftmax` and `F.log_softmax` you'll need to set the `dim` keyword argument appropriately. `dim=0` calculates softmax across the rows, so each column sums to 1, while `dim=1` calculates across the columns so each row sums to 1. Think about what you want the output to be and choose `dim` appropriately.\n"
      ],
      "metadata": {
        "id": "cdlcSnVD3Nw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "079m95lo5JZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lzRkHaYhb1sq"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) Debug the training\n",
        "With the default parameters we provided above, you should get a validation accuracy of about 0.2 on the validation set. This isn't very good.\n",
        "\n",
        "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization. \n",
        "\n",
        "**Exersice:** Write this visualization here.\n",
        "(you can copy your solution from last exercies)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u2Bwq6QeD8ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNP6Ogkxb1sw"
      },
      "cell_type": "markdown",
      "source": [
        "# 4) Tune your hyperparameters\n",
        "\n",
        "**What's wrong?**. Looking at the loss above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy. \n",
        "\n",
        "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including *hidden layer size, learning rate, numer of training epochs, and adding l2 or l1 regularization to the cross entropy loss*. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
        "\n",
        "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.\n",
        "\n",
        "**Experiment**: You goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.)."
      ]
    },
    {
      "metadata": {
        "id": "5ITsPTx6b1sz"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#################################################################################\n",
        "# TODO: Tune hyperparameters using the validation set.                          #\n",
        "#                                                                               #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "# Your code\n",
        "#################################################################################\n",
        "#                               END OF YOUR CODE                                #\n",
        "#################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N40TSSK0b1s-"
      },
      "cell_type": "markdown",
      "source": [
        "# Interview Question\n",
        "\n",
        "Now that you have trained a Neural Network classifier, you may find that your testing accuracy is lower than the training accuracy. In what ways can we decrease this gap? Select all that apply.\n",
        "1. Train on a larger dataset.\n",
        "2. Add more hidden units.\n",
        "3. Increase the regularization strength.\n",
        "4. None of the above.\n",
        "\n",
        "*Your answer*:\n",
        "\n",
        "*Your explanation:*"
      ]
    }
  ]
}